{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMdistance_update\n",
    "1. Delete all the irrelevant or depleted functions such as functions related to preparation of noise as input, optimization, reconstruction and output. Content loss should be discarded in the future as well since it is a bit redundant. \n",
    "2. Create a new function to prepare input.\n",
    "3. Modified run_style_transfer. The biggest change is that there is no need to iterate runs, just six streams, so the closure function is no longer needed. \n",
    "\n",
    "======================================================================================================================\n",
    "Until this point, the skeleton only invloves two input files, namely \"style_img\" and \"input_img\". The output generated is a floating number. The second session is the test on different pop dataset to verify the hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.nn.modules.module import _addindent\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import math\n",
    "\n",
    "#for random feature projection\n",
    "from scipy.stats import ortho_group\n",
    "\n",
    "\n",
    "import soundfile as sf\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original functions to prepare the input. Here, numsteps=1 since we are not training the input image. These functions will combine together in the *prepare_input* function in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_scale(img):\n",
    "    img = np.log1p(img)\n",
    "    return img\n",
    "\n",
    "def inv_log(img):\n",
    "    img = np.exp(img) - 1.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True uses PGHI (Truncated Gaussian window, log mag, PGHI reconstruction)\n",
    "# False uses Log Mag spectrum and Griffin Lim reconstruction\n",
    "tifresi=False\n",
    "\n",
    "STYLE_FILENAME = \"inputs/Pops/popb0.wav\" #input filename\n",
    "INPUT_FILENAME = \"inputs/Pops/popc0.wav\"\n",
    "# You can have your output length be anything! \n",
    "SYNTHETIC_LENGTH_FACTOR = 1 # duration of generated relative to target texture\n",
    "\n",
    "#Go up to 20000 for high quality if you are on a GPU\n",
    "num_steps=1 #00 #70\n",
    "\n",
    "####################################################################################################\n",
    "#Gram matrix dot product multiplies: RFMSTACK*(feature_size/RFMSALEFACTOR)^2\n",
    "RANDOM_PROJECTION= \"Gaussian\" # \"None\", \"Gaussian\", \"Sparse\"   (n=None means no random projection at all)\n",
    "RFMSCALEFACTOR=16   #RM project to size relative to feature dimension\n",
    "RFMSTACK=16         # how many different RMs to stack\n",
    "####################################################################################################\n",
    "\n",
    "#MS Number of separate CNNs operating on input\n",
    "numStreams=6\n",
    "learning_Rate= 1 #0.1 #.1  # must be smaller for more streams (if numStream=1, then learning_Rate can be 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filenames\n",
    "outname = 'rftests' #output dir\n",
    "runs = 1 # no. of separate outputs (to test consistency), 1 if only want single output\n",
    "\n",
    "#librosa audio params\n",
    "N_FFT = 512 \n",
    "K_HOP = 128 \n",
    "\n",
    "# architecture\n",
    "\"\"\"use a custom convolutional network randomly initialized\"\"\" \n",
    "use01scale = False #set to true if want to scale img to [0,1] prior to training. Recommended if using VGG19\n",
    "boundopt = False #whether to regularize the input within [lower,upper]. Recommended if using VGG19\n",
    "whichChannel = \"freq\" #2d=2d conv, 1d options:freq=freq bins as channels, time= time bins as channels \n",
    "N_FILTERS = 512# 4096 #no. of filters in 1st conv layer\n",
    "# hor_filter = 5 #width of conv filter, for 2d also the height of (square) kernel\n",
    "possible_kernels = [2,4,8,16,64,128,256,512,1024,2048]\n",
    "# possible_kernels = [2,4,8,16,32,64,128,256,512,1024,2048]\n",
    "# possible_kernels = [5,5,5,5,5,5]\n",
    "hor_filters = [0]*numStreams\n",
    "for j in range(numStreams):\n",
    "    hor_filters[j]=possible_kernels[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifresi.hparams import HParams\n",
    "from tifresi.stft import GaussTruncTF\n",
    "\n",
    "# from tifresi.transforms import log_spectrogram\n",
    "# from tifresi.transforms import inv_log_spectrogram\n",
    "#NOTE: Not using Marifioties 10 log_10 transform. Instead use natural log.\n",
    "# This makes a HUGE difference !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "log_spectrogram=log_scale\n",
    "inv_log_spectrogram=inv_log\n",
    "\n",
    "stft_channels = HParams.stft_channels = N_FFT \n",
    "hop_size =  HParams.hop_size  = K_HOP\n",
    "HParams.sr=16000 \n",
    "\n",
    "# empirically set: - too small, get low-res ringing; too high, get distortion\n",
    "tfresiMagSpectScale=5 # Takes the [0,1] mag spectrogram and maps it to [0, tfresiMagSpectScale]\n",
    "\n",
    "# For faster processin, a truncated window can be used instead\n",
    "stft_system = GaussTruncTF(hop_size=hop_size, stft_channels=stft_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available = False\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available() #use GPU if available\n",
    "print('GPU available =',use_cuda)\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio_spectum(filename, tifresi=True):\n",
    "    #x, fs = librosa.load(filename) #x=audiodata, fs=samplerate\n",
    "    x, fs  = sf.read(filename)\n",
    "#    print(f'input file sample rate = {fs}')\n",
    "#     x=np.append(x, np.zeros(stft_channels-np.mod(len(x), stft_channels)))\n",
    "    N_SAMPLES = len(x)\n",
    "#     x=x+np.random.normal(scale=.0001, size=N_SAMPLES)\n",
    "#   print(f'np.mod(len(x), stft_channels) = {np.mod(len(x), stft_channels)}')\n",
    "#   print(f' number of hops is {len(x)//hop_size}')\n",
    "    \n",
    "    if tifresi :\n",
    "#       print(f'In read audio, doing tifresi spectrogram')\n",
    "#         R=tfresiMagSpectScale*stft_system.spectrogram(x)\n",
    "        R=stft_system.spectrogram(x)\n",
    "    else :\n",
    "#       print(f'In read audio, doing librosa spectrogram')\n",
    "        R = np.abs(librosa.stft(x, n_fft=N_FFT, hop_length=K_HOP, win_length=N_FFT,  center=False))    \n",
    "        \n",
    "#   print(f'R range is  [{np.amin(R)}, {np.amax(R)}')\n",
    "#   print(f'K_HOP - {K_HOP} and N_FFT is {N_FFT}')\n",
    "#   print(f'R shape is {R.shape}')\n",
    "    return R, fs\n",
    "\n",
    "\n",
    "def findMinMax(img):\n",
    "    return int(math.floor(np.amin(img))),int(math.ceil(np.amax(img)))\n",
    "\n",
    "def img_scale(img,datasetMin,datasetMax,scaleMin,scaleMax):\n",
    "    \"\"\"scales input numpy array from [datasetMin,datasetMax] -> [scaleMin,scaleMax]\"\"\"    \n",
    "    shift = (scaleMax-scaleMin) / (datasetMax-datasetMin)\n",
    "    scaled_values = shift * (img-datasetMin) + scaleMin\n",
    "#   print(\"img_scale: Using [{0},{1}] -> [{2},{3}] for scale conversion\".format(datasetMin,datasetMax,scaleMin,scaleMax))\n",
    "    return scaled_values\n",
    "\n",
    "def img_invscale(img,datasetMin,datasetMax,scaleMin,scaleMax):\n",
    "    \"\"\"scales input numpy array from [scaleMin,scaleMax] -> [datasetMin,datasetMax]\"\"\"\n",
    "    shift = (datasetMax-datasetMin) / (scaleMax-scaleMin)\n",
    "    scaled_values = shift * (img-scaleMin) + datasetMin\n",
    "#   print(\"img_invscale: Using [{0},{1}] -> [{2},{3}] for inverse scale conversion\".format(scaleMin,scaleMax,datasetMin,datasetMax))\n",
    "    return scaled_values\n",
    "    \n",
    "#if 0\n",
    "    # use below functions to use librosa db scale, normalized to [0,1]\n",
    "    # note that this scaling does not work well for style transfer\n",
    "    def db_scale(img,scale=80):\n",
    "        img = librosa.amplitude_to_db(img)\n",
    "        shift = float(np.amax(img))\n",
    "        img = img - shift #put max at 0\n",
    "        img = img/scale #scale from [-80,0] to [-1,0]\n",
    "        img = img + 1. #shift to [0,1]\n",
    "        img = np.maximum(img, 0) #clip anything below 0\n",
    "        return img, shift\n",
    "\n",
    "    def inv_db(img,shift,scale=80):\n",
    "        img = img - 1. #shift from [0,1] to [-1,0]\n",
    "        img = img * scale #[-1,0] -> [-80,0]\n",
    "        img = img + shift\n",
    "        img = librosa.db_to_amplitude(img)    \n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the function to prepare the input. The two inputs will be pre-processed thorugh the same function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new function to prepare and inspect the inputs. \n",
    "def prepare_input(FILENAME):\n",
    "    R, fs = read_audio_spectum(FILENAME, tifresi)\n",
    "    print(f\"raw spectrogram R range before log and scaling: [{np.amin(R)},{np.amax(R)}] \")\n",
    "\n",
    "    if tifresi :\n",
    "        a_style = log_spectrogram(R)\n",
    "    else :\n",
    "        a_style = log_scale(R)\n",
    "    \n",
    "    print(f\"    LOG    range before scaling: [{np.amin(a_style)},{np.amax(a_style)}]\",)\n",
    "    print(f'shape of a_style is {a_style.shape}')\n",
    "\n",
    "    if use01scale == True:\n",
    "        a_min,a_max = findMinMax(a_style)\n",
    "        a_style = img_scale(a_style,a_min,a_max,0,1)\n",
    "    \n",
    "    if 0 : #RUN PGHI ON THE ORIGINAL INPUT DATA SPECTROGRAM  - ASSUMING TFRESI for the little experiment in this cell\n",
    "    # now invert it, just like we will do with the GAN-generated images\n",
    "\n",
    "        #SPECTOFFSET\n",
    "        SPECTOFFSET=0 #.25\n",
    "\n",
    "        lspect=a_style\n",
    "        print(f' shape of lspect is {lspect.shape}')\n",
    "        print(use01scale)\n",
    "\n",
    "        #ASSUMING TIFRES\n",
    "        print('TIFResi: inv_log_spectrogram')\n",
    "        if use01scale == True:\n",
    "            out_spec = inv_log_spectrogram(lspect+ SPECTOFFSET) \n",
    "        else:\n",
    "            out_spec =inv_log_spectrogram(lspect+SPECTOFFSET) \n",
    "        print(f' shape of out_spec is {out_spec.shape}')\n",
    "        x = stft_system.invert_spectrogram(out_spec) \n",
    "        display(Audio(x, rate=16000, autoplay=True))\n",
    "\n",
    "    #plt.figure(figsize=(10, 5))\n",
    "    #plt.title('Style spectrogram')\n",
    "    #plt.imshow(a_style, origin='lower')\n",
    "    temp_a_style=a_style # for reality check in next cell\n",
    "\n",
    "    N_SAMPLES = a_style.shape[1] #time bins\n",
    "    N_FREQ = a_style.shape[0] #freq bins\n",
    "    \n",
    "    a_style = np.ascontiguousarray(a_style[None,None,:,:]) #[batch,channels,freq,samples]\n",
    "    if whichChannel == \"2d\":\n",
    "        a_style = torch.from_numpy(a_style) #pytorch:[batch,channels(1),height(freq),width(samples)]\n",
    "    elif whichChannel == \"freq\":\n",
    "        a_style = torch.from_numpy(a_style).permute(0,2,1,3) #pytorch:[batch,channels(freq),height(1),width(samples)]\n",
    "    elif whichChannel == \"time\":\n",
    "        a_style = torch.from_numpy(a_style).permute(0,3,1,2) #pytorch:[batch,channels(samples),height(1),width(freq)]\n",
    "\n",
    "    converted_img = Variable(a_style).type(dtype) #convert to pytorch variable\n",
    "    return converted_img,N_SAMPLES,N_FREQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw spectrogram R range before log and scaling: [0.0,43.46055766825932] \n",
      "    LOG    range before scaling: [0.0,3.7946024516031196]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,28.47720488931677] \n",
      "    LOG    range before scaling: [0.0,3.3836172490307987]\n",
      "shape of a_style is (257, 247)\n"
     ]
    }
   ],
   "source": [
    "style_img,N_SAMPLES,N_FREQ=prepare_input(STYLE_FILENAME)\n",
    "input_img,n_SAMPLES,n_FREQ=prepare_input(INPUT_FILENAME) #create two dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257, 247)\n",
      "Output range: 0.0 3.7946024\n",
      "mean and sigma: 0.090404764 0.30420274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(tensor, n_freqs, n_samples, channels=1, title=None, origin='lower'):\n",
    "    \"\"\" transforms pytorch tensor to numpy array and plots it out \"\"\"\n",
    "    \n",
    "    image = tensor.clone().cpu()  # we clone the tensor to not do changes on it\n",
    "    if whichChannel == \"2d\":\n",
    "        image = image\n",
    "    if whichChannel == \"freq\":\n",
    "        image = image.permute(0,2,1,3).contiguous() # get the dimensions in proper order\n",
    "    elif whichChannel == \"time\":\n",
    "        image = image.permute(0,2,3,1).contiguous() # get the dimensions in proper order\n",
    "    image = image.view(n_freqs, n_samples)  # remove the fake batch dimension\n",
    "    \n",
    "    image = image.numpy() #convert pytorch tensor to numpy array\n",
    "    print(image.shape) #check shape\n",
    "    print(\"Output range:\",np.amin(image),np.amax(image))\n",
    "    print(\"mean and sigma:\",np.mean(image),np.std(image))\n",
    "    \n",
    "    \n",
    "    plt.imshow(image, origin=origin)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) # pause a bit so that plots are updated\n",
    "    return image #return numpy array\n",
    "\n",
    "\"Just to check we can get back the numpy array with the correct shape\"\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.figure()\n",
    "_=imshow(style_img.data, N_FREQ, N_SAMPLES, title='Style Spectrogram', origin='lower')\n",
    "\n",
    "#plt.figure()\n",
    "#imshow(content_img.data, title='Content Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network begins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False), ReLU()]\n",
      "[Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False), ReLU()]\n",
      "[Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False), ReLU()]\n",
      "[Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False), ReLU()]\n",
      "[Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False), ReLU()]\n",
      "[Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False), ReLU()]\n"
     ]
    }
   ],
   "source": [
    "\"Here we create the custom network\"\n",
    "import collections as c\n",
    "\n",
    "if whichChannel == \"2d\":\n",
    "    IN_CHANNELS = 1\n",
    "elif whichChannel == \"freq\":\n",
    "    IN_CHANNELS = N_FREQ\n",
    "elif whichChannel == \"time\":\n",
    "    IN_CHANNELS = N_SAMPLES\n",
    "    \n",
    "\n",
    "# custom weights initialization\n",
    "def weights_init(m,hor_filter):\n",
    "    std = np.sqrt(2) * np.sqrt(2.0 / ((N_FREQ + N_FILTERS) * hor_filter))\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "#         torch.nn.init.xavier_uniform_(m.weight)\n",
    "#         m.bias.data.fill_(0.01)\n",
    "        m.weight.data.normal_(0.0, std)\n",
    "\n",
    "class style_net(nn.Module):\n",
    "    \"\"\"Here create the network you want to use by adding/removing layers in nn.Sequential\"\"\"\n",
    "    def __init__(self,hor_filter):\n",
    "        super(style_net, self).__init__()\n",
    "#         self.hor_filter=hor_filter\n",
    "        self.layers = nn.Sequential(c.OrderedDict([\n",
    "                            ('conv1',nn.Conv2d(IN_CHANNELS,N_FILTERS,kernel_size=(1,hor_filter),bias=False)),\n",
    "                            ('relu1',nn.ReLU())#,\n",
    "#                             ('max1', nn.MaxPool2d(kernel_size=(1,2))),\n",
    "            \n",
    "#                             ('conv2',nn.Conv2d(N_FILTERS,N_FILTERS//2,kernel_size=(1,hor_filter),bias=False)),\n",
    "#                             ('relu2',nn.ReLU()),\n",
    "#                             ('max2', nn.MaxPool2d(kernel_size=(1,2))),\n",
    "            \n",
    "#                             ('conv3',nn.Conv2d(N_FILTERS//2,N_FILTERS//4,kernel_size=(1,hor_filter),bias=False)),\n",
    "#                             ('relu3',nn.ReLU())\n",
    "        ]))\n",
    "\n",
    "                            #('relu2',nn.ReLU()),\n",
    "                            #('max2', nn.MaxPool2d(kernel_size=(1,2))),\n",
    "\n",
    "\n",
    "                            #('conv3',nn.Conv2d(N_FILTERS//2,N_FILTERS//2,kernel_size=(1,hor_filter),bias=False)),\n",
    "                            #('relu3',nn.ReLU()),\n",
    "                            #('max3', nn.MaxPool2d(kernel_size=(1,2))),\n",
    "\n",
    "                            #('conv4',nn.Conv2d(N_FILTERS//2,N_FILTERS//4,kernel_size=(1,hor_filter),bias=False)),\n",
    "                            #('relu4',nn.ReLU()),\n",
    "                            #('max4', nn.MaxPool2d(kernel_size=(1,2))),\n",
    "\n",
    "\n",
    "                            #('conv5',nn.Conv2d(N_FILTERS//4,N_FILTERS//4,kernel_size=(1,hor_filter),bias=False)),\n",
    "                            #('relu5',nn.ReLU()),\n",
    "                            #('max5', nn.MaxPool2d(kernel_size=(1,2))),\n",
    "\n",
    "\n",
    "                            #('conv6',nn.Conv2d(N_FILTERS//4,N_FILTERS//8,kernel_size=(1,hor_filter),bias=False)),\n",
    "                            #('relu6',nn.ReLU())]))\n",
    "\n",
    "    def forward(self,input):\n",
    "        out = self.layers(input)\n",
    "        return out\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    \n",
    "cnnlist=[] \n",
    "#MS create a separate CNN for each stream\n",
    "for j in range(numStreams) :\n",
    "    cnn = style_net(hor_filters[j])\n",
    "    cnn.apply(lambda x, f=hor_filters[j]: weights_init(x,f))\n",
    "    for param in cnn.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(list(cnn.layers))\n",
    "\n",
    "    # move it to the GPU if possible:\n",
    "    if use_cuda:\n",
    "        cnn = cnn.cuda()\n",
    "    \n",
    "    cnnlist.append(cnn)\n",
    "\n",
    "# Add the style/content loss 'layer' after the specified layer:\n",
    "content_layers_default = [] #ignore for now\n",
    "# style_layers_default = ['relu_1']\n",
    "style_layers_default = ['relu_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    \"\"\"Since doing texture only we ignore this for now\"\"\"\n",
    "\n",
    "    def __init__(self, target, weight): #weight here is the epsilon tuning (how much content vs style)\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # we 'detach' the target content from the tree used\n",
    "        self.target = target.detach() * weight\n",
    "        # to dynamically compute the gradient: this is a stated value,\n",
    "        # not a variable. Otherwise the forward method of the criterion\n",
    "        # will throw an error.\n",
    "        self.weight = weight\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = self.criterion(input * self.weight, self.target)\n",
    "        self.output = input\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, retain_variables=True):\n",
    "        self.loss.backward(retain_variables=retain_variables)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "# create random feature projection matrix\n",
    "RFM={}\n",
    "\n",
    "# This is the \"sparse\" Achlioptas matrix\n",
    "if RANDOM_PROJECTION==\"Sparse\" : \n",
    "    RFM[1024]=torch.from_numpy(np.random.choice(np.sqrt(3)*np.array([1., 0., -1.]), (1024,int(1024/RFMSCALEFACTOR)), p=[1./6.,2./3., 1./6.])).type(dtype)\n",
    "    RFM[512]=torch.from_numpy(np.random.choice(np.sqrt(3)*np.array([1., 0., -1.]), (512,int(512/RFMSCALEFACTOR)), p=[1./6.,2./3., 1./6.])).type(dtype)\n",
    "    RFM[256]=torch.from_numpy(np.random.choice(np.sqrt(3)*np.array([1., 0., -1.]), (256,int(256/RFMSCALEFACTOR)), p=[1./6.,2./3., 1./6.])).type(dtype)\n",
    "    RFM[128]=torch.from_numpy(np.random.choice(np.sqrt(3)*np.array([1., 0., -1.]), (128,int(128/RFMSCALEFACTOR)), p=[1./6.,2./3., 1./6.])).type(dtype)\n",
    "\n",
    "#Full orthonormal gaussian \n",
    "if RANDOM_PROJECTION==\"Gaussian\" : \n",
    "   # RFM[1024]=(torch.from_numpy(ortho_group.rvs(1024)[:int(1024/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)\n",
    "    RFM[512]= (torch.from_numpy(ortho_group.rvs(512)[: int(512/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)\n",
    "    #RFM[256]= (torch.from_numpy(ortho_group.rvs(256)[: int(256/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)\n",
    "    #RFM[128]= (torch.from_numpy(ortho_group.rvs(128)[: int(128/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)\n",
    "    for rm in range(1, RFMSTACK) : \n",
    "        #RFM[1024]= torch.cat( (RFM[1024], (torch.from_numpy(ortho_group.rvs(1024)[:int(1024/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)), dim=0)\n",
    "        RFM[512]=torch.cat( (RFM[512],    (torch.from_numpy(ortho_group.rvs(512)[ :int(512/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)), dim=0)\n",
    "        #RFM[256]=torch.cat( (RFM[256],    (torch.from_numpy(ortho_group.rvs(256)[ :int(256/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)), dim=0)\n",
    "        #RFM[128]=torch.cat( (RFM[128],    (torch.from_numpy(ortho_group.rvs(128)[ :int(128/RFMSCALEFACTOR)]).type(dtype)).unsqueeze(0)), dim=0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "\n",
    "    def forward(self, input):\n",
    "        a, b, c, d = input.size() #a=batch size(=1)\n",
    "                                  #b=number of feature maps\n",
    "                                  #(c,d)=dimensions of a feat. map (N=c*d) -> for 1D conv c=1\n",
    "        #features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
    "        features = input.view(b, a * c * d)  # resise F_XL into \\hat F_XL\n",
    "        \n",
    "       # features2=torch.matmul(RFM[:b//2,:b], features)\n",
    "        if RANDOM_PROJECTION==\"None\":\n",
    "            features2=features.unsqueeze(0)\n",
    "        else :\n",
    "            features2=torch.matmul(RFM[b], features)\n",
    "        #print(f\"In GramMatrix, shape of features = {np.shape(features2)}\")\n",
    "\n",
    "        #G = torch.matmul(features2, features2.t())  # compute the gram product #normalizing? - length of the features\n",
    "        G = torch.matmul(features2, torch.transpose(features2, 1,2))  # compute the gram product #normalizing? - length of the features\n",
    "        #print(f\"In GramMatrix, shape of grammatrix = {np.shape(G)}\")\n",
    "        # we 'normalize' the values of the gram matrix\n",
    "        # by dividing by the number of element in each feature maps.\n",
    "        return G.div(a * c * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foofoo\n",
    "class StyleLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target, weight, layer): #weight here is the alpha tuning (how much content vs style)\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = target.detach() * weight\n",
    "        self.weight = weight\n",
    "        self.gram = GramMatrix()\n",
    "        self.criterion = nn.MSELoss(size_average=False)\n",
    "        #print(f'Creating StyleLoss module - this one for layer {layer} with target shape {target.shape}' )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print(f' StyleLoss.forward with input of shape {input.shape}')\n",
    "        self.output = input.clone()\n",
    "        self.G = self.gram(input)\n",
    "        self.G.mul_(self.weight)\n",
    "        self.loss = self.criterion(self.G, self.target) #/sum(sum(self.target**2)) #target=gram mat for style img, G=gram mat for current input ie. noise\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, retain_variables=True):\n",
    "        self.loss.backward(retain_graph=True)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebuild network with the layers we want\n",
    "def get_style_model_and_losses(cnn, style_img, content_img=None,\n",
    "                               style_weight=1, content_weight=0,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default, style_img2=None):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    # just in order to have an iterable access to or list of content/syle losses\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "    \n",
    "    model = nn.Sequential()\n",
    "    layer_list = list(cnn.layers)\n",
    "    \n",
    "    gram = GramMatrix()  # we need a gram module in order to compute style targets\n",
    "\n",
    "    # move these modules to the GPU if possible:\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        gram = gram.cuda()\n",
    "\n",
    "    #here we rebuild the network adding the in content and style loss \"layers\"   \n",
    "    i = 1  \n",
    "    for layer in layer_list:\n",
    "        \n",
    "        if isinstance(layer, nn.Conv2d): #if layer in vgg19 belong to class nn.Conv2d\n",
    "            name = \"conv_\" + str(i)\n",
    "            model.add_module(name, layer) #add that layer to our sequential model\n",
    "            \n",
    "            if content_img != None:\n",
    "                if name in content_layers: #at the right depth add the content loss \"layer\"\n",
    "                    # add content loss:\n",
    "                    target = model(content_img).clone()\n",
    "                    content_loss = ContentLoss(target, content_weight)\n",
    "                    model.add_module(\"content_loss_\" + str(i), content_loss)\n",
    "                    content_losses.append(content_loss)\n",
    "\n",
    "            if name in style_layers: #at the right depth add the content loss \"layer\"\n",
    "                # add style loss:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                style_loss = StyleLoss(target_feature_gram, style_weight, name)\n",
    "                model.add_module(\"style_loss_\" + str(i), style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "\n",
    "        if isinstance(layer, nn.ReLU): #do the same for ReLUs\n",
    "            name = \"relu_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "            \n",
    "            if content_img != None:\n",
    "                if name in content_layers:\n",
    "                    # add content loss:\n",
    "                    target = model(content_img).clone()\n",
    "                    content_loss = ContentLoss(target, content_weight)\n",
    "                    model.add_module(\"content_loss_\" + str(i), content_loss)\n",
    "                    content_losses.append(content_loss)\n",
    "\n",
    "            if name in style_layers:\n",
    "                # add style loss:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                style_loss = StyleLoss(target_feature_gram, style_weight, name)\n",
    "                model.add_module(\"style_loss_\" + str(i), style_loss)\n",
    "                style_losses.append(style_loss) \n",
    "                \n",
    "            i += 1\n",
    "\n",
    "        if isinstance(layer, nn.MaxPool2d): #do the same for maxpool\n",
    "            name = \"pool_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "            \n",
    "            if name in style_layers:\n",
    "                # add style loss:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                style_loss = StyleLoss(target_feature_gram, style_weight, name)\n",
    "                model.add_module(\"style_loss_\" + str(i), style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "            \n",
    "            #avgpool = nn.AvgPool2d(kernel_size=(1,2),\n",
    "            #                stride=layer.stride, padding = layer.padding)\n",
    "            #model.add_module(name, avgpool)  # *** can also replace certain layers if we want eg. maxpool -> avgpool\n",
    "\n",
    "\n",
    "    #for param in model.parameters():\n",
    "    #    param.requires_grad = False\n",
    "    return model, style_losses,target_feature_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer(cnnlist, style_img, input_img, content_img=None, num_steps=num_steps,\n",
    "                       style_weight=1, content_weight=0): \n",
    "    \"\"\"Run the style transfer\"\"\"\n",
    "    \n",
    "    #MS - one model, loss accumulator, and optimizer per stream \n",
    "    \n",
    "    modelMS=[None for j in range(numStreams)]\n",
    "    style_lossesMS=[None for j in range(numStreams)] \n",
    "    content_lossesMS=[None for j in range(numStreams)]\n",
    "\n",
    "\n",
    "    #MS one input_param for all streams, each operating on the same data. \n",
    "    input_param = nn.Parameter(input_img.data)\n",
    "    \n",
    "    \n",
    "    prev = input_param.data\n",
    "    \n",
    "    # first create the separate cnn models and losses\n",
    "    for j in range(numStreams) :\n",
    "        print('Building the style transfer model..')\n",
    "        modelMS[j], style_lossesMS[j], content_lossesMS[j] = get_style_model_and_losses(cnnlist[j],\n",
    "            style_img, content_img, style_weight, content_weight)\n",
    "\n",
    "        print(\"Input range:\",torch.max(input_param.data),torch.min(input_param.data))   \n",
    "        print(modelMS[j])\n",
    "    \n",
    "    print(f'Created {j} MS models')\n",
    "    # print(f' input_param[0] is input_param[1] ? ...... {input_paramMS[0] is input_paramMS[1]}')    # FALSE!    \n",
    "    \n",
    "\n",
    "    run = [0]\n",
    "    \n",
    "    plt.figure()\n",
    "    while run[0] < num_steps:\n",
    "\n",
    "        run[0] += 1\n",
    "        print(\"Current Run Number = \",run[0])\n",
    "        \n",
    "        #MS create the closure, and then run the optimizer for each stream in sequence\n",
    "        # (the update from each stream becomes the input for the next, I think!)\n",
    "        print(\"numStreams = \",numStreams)\n",
    "        #def closure():\n",
    "        style_score = 0\n",
    "        for j in range(numStreams) :       \n",
    "            modelMS[j](input_param)                \n",
    "            for sl in style_lossesMS[j]:\n",
    "                style_score += sl.backward() #call backward method to grab the loss\n",
    "            print(\"run\", run[0], \" stream \",j)          \n",
    "            total_loss = style_score\n",
    "            print(\"total_loss={:4f}\".format(total_loss))\n",
    "            \n",
    "        # END MS loop\n",
    "        \n",
    "    return total_loss/numStreams   # Can return any of the MS input_params - they all refer to the same object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the loss in two way\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "def visualize_loss(s,l):\n",
    "    fig, ax = plt.subplots()\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='50%', pad=0.05)\n",
    "    im = ax.imshow(s, cmap='bone')\n",
    "    fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "    plt.show()\n",
    "    \n",
    "    # plotting the loss\n",
    "    plt.title(\"Loss graph\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(l, color =\"red\")\n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['popcf02.wav', 'popcf16.wav', 'popb2.wav', 'popb3.wav', 'popcf17.wav', 'popcf15.wav', 'popcf01.wav', 'popb1.wav', 'popb0.wav', 'popc4.wav', 'popcf00.wav', 'popcf14.wav', 'popcf10.wav', 'popcf04.wav', '.DS_Store', 'popc0.wav', 'popb4.wav', 'popc1.wav', 'popcf05.wav', 'popcf11.wav', 'popcf07.wav', 'popcf13.wav', 'popcf03wav.wav', 'popc3.wav', 'popc2.wav', 'popcf12.wav', 'popcf06.wav', 'popd1.wav', 'popd0.wav', 'popd2.wav', 'popd3.wav', 'popd7.wav', 'popd6.wav', 'popd4.wav', 'popd5.wav', '.ipynb_checkpoints', 'popcf20.wav', 'popcf08.wav', 'popcf09.wav', 'popcf19.wav', 'popcf18.wav']\n",
      "input_imgs: ['inputs/Pops/popcf00.wav', 'inputs/Pops/popcf01.wav', 'inputs/Pops/popcf02.wav', 'inputs/Pops/popcf03wav.wav', 'inputs/Pops/popcf04.wav', 'inputs/Pops/popcf05.wav', 'inputs/Pops/popcf06.wav', 'inputs/Pops/popcf07.wav', 'inputs/Pops/popcf08.wav', 'inputs/Pops/popcf09.wav', 'inputs/Pops/popcf10.wav', 'inputs/Pops/popcf11.wav', 'inputs/Pops/popcf12.wav', 'inputs/Pops/popcf13.wav', 'inputs/Pops/popcf14.wav', 'inputs/Pops/popcf15.wav', 'inputs/Pops/popcf16.wav', 'inputs/Pops/popcf17.wav', 'inputs/Pops/popcf18.wav', 'inputs/Pops/popcf19.wav', 'inputs/Pops/popcf20.wav']\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7683) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7683) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7683) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7683) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7683) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input range: tensor(3.7683) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=0.000000\n",
      "run 1  stream  1\n",
      "total_loss=0.000000\n",
      "run 1  stream  2\n",
      "total_loss=0.000000\n",
      "run 1  stream  3\n",
      "total_loss=0.000000\n",
      "run 1  stream  4\n",
      "total_loss=0.000000\n",
      "run 1  stream  5\n",
      "total_loss=0.000000\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,43.09821209855036] \n",
      "    LOG    range before scaling: [0.0,3.786419239647967]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7864) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7864) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7864) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7864) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7864) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7864) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=0.381447\n",
      "run 1  stream  1\n",
      "total_loss=0.652804\n",
      "run 1  stream  2\n",
      "total_loss=0.886626\n",
      "run 1  stream  3\n",
      "total_loss=1.030142\n",
      "run 1  stream  4\n",
      "total_loss=1.165448\n",
      "run 1  stream  5\n",
      "total_loss=1.355583\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,41.684952990992294] \n",
      "    LOG    range before scaling: [0.0,3.7538464691612603]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7538) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7538) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7538) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7538) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7538) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7538) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=0.453436\n",
      "run 1  stream  1\n",
      "total_loss=0.904220\n",
      "run 1  stream  2\n",
      "total_loss=1.241223\n",
      "run 1  stream  3\n",
      "total_loss=1.416314\n",
      "run 1  stream  4\n",
      "total_loss=1.580877\n",
      "run 1  stream  5\n",
      "total_loss=1.830273\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,40.6226253404484] \n",
      "    LOG    range before scaling: [0.0,3.728643897795142]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7286) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7286) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7286) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7286) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7286) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7286) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=1.077983\n",
      "run 1  stream  1\n",
      "total_loss=1.872305\n",
      "run 1  stream  2\n",
      "total_loss=2.486121\n",
      "run 1  stream  3\n",
      "total_loss=2.825599\n",
      "run 1  stream  4\n",
      "total_loss=3.040143\n",
      "run 1  stream  5\n",
      "total_loss=3.293370\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,37.40841952142768] \n",
      "    LOG    range before scaling: [0.0,3.6482766939305904]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6483) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6483) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6483) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6483) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6483) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6483) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=1.779873\n",
      "run 1  stream  1\n",
      "total_loss=3.207582\n",
      "run 1  stream  2\n",
      "total_loss=4.372159\n",
      "run 1  stream  3\n",
      "total_loss=4.993231\n",
      "run 1  stream  4\n",
      "total_loss=5.375917\n",
      "run 1  stream  5\n",
      "total_loss=5.779518\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,41.06897132961946] \n",
      "    LOG    range before scaling: [0.0,3.739310445903904]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7393) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7393) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7393) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7393) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7393) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7393) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=1.131819\n",
      "run 1  stream  1\n",
      "total_loss=1.929987\n",
      "run 1  stream  2\n",
      "total_loss=2.529899\n",
      "run 1  stream  3\n",
      "total_loss=2.845937\n",
      "run 1  stream  4\n",
      "total_loss=3.092829\n",
      "run 1  stream  5\n",
      "total_loss=3.389828\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,39.5668749623201] \n",
      "    LOG    range before scaling: [0.0,3.702951845962644]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7030) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7030) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7030) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7030) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7030) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7030) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=1.687335\n",
      "run 1  stream  1\n",
      "total_loss=2.983680\n",
      "run 1  stream  2\n",
      "total_loss=3.972194\n",
      "run 1  stream  3\n",
      "total_loss=4.537580\n",
      "run 1  stream  4\n",
      "total_loss=4.926077\n",
      "run 1  stream  5\n",
      "total_loss=5.347662\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,35.389683897232395] \n",
      "    LOG    range before scaling: [0.0,3.5942853250579643]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5943) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5943) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5943) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5943) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5943) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5943) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=2.923963\n",
      "run 1  stream  1\n",
      "total_loss=4.826741\n",
      "run 1  stream  2\n",
      "total_loss=6.330585\n",
      "run 1  stream  3\n",
      "total_loss=7.151395\n",
      "run 1  stream  4\n",
      "total_loss=7.674938\n",
      "run 1  stream  5\n",
      "total_loss=8.180125\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,35.723289561745865] \n",
      "    LOG    range before scaling: [0.0,3.6034111467546577]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6034) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6034) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6034) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6034) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6034) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6034) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=2.945144\n",
      "run 1  stream  1\n",
      "total_loss=5.114954\n",
      "run 1  stream  2\n",
      "total_loss=6.800488\n",
      "run 1  stream  3\n",
      "total_loss=7.676475\n",
      "run 1  stream  4\n",
      "total_loss=8.215101\n",
      "run 1  stream  5\n",
      "total_loss=8.799236\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,34.680244806809604] \n",
      "    LOG    range before scaling: [0.0,3.57459716886567]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5746) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5746) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5746) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5746) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5746) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5746) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=4.088964\n",
      "run 1  stream  1\n",
      "total_loss=6.619294\n",
      "run 1  stream  2\n",
      "total_loss=8.512196\n",
      "run 1  stream  3\n",
      "total_loss=9.599327\n",
      "run 1  stream  4\n",
      "total_loss=10.265641\n",
      "run 1  stream  5\n",
      "total_loss=10.933002\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,36.27288416804387] \n",
      "    LOG    range before scaling: [0.0,3.6182660962776665]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6183) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6183) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6183) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6183) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6183) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.6183) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=4.928477\n",
      "run 1  stream  1\n",
      "total_loss=7.995073\n",
      "run 1  stream  2\n",
      "total_loss=10.191659\n",
      "run 1  stream  3\n",
      "total_loss=11.376591\n",
      "run 1  stream  4\n",
      "total_loss=12.068005\n",
      "run 1  stream  5\n",
      "total_loss=12.766156\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,32.97408676372851] \n",
      "    LOG    range before scaling: [0.0,3.525598080021153]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5256) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5256) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5256) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5256) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5256) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5256) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=5.017365\n",
      "run 1  stream  1\n",
      "total_loss=8.266722\n",
      "run 1  stream  2\n",
      "total_loss=10.639025\n",
      "run 1  stream  3\n",
      "total_loss=11.864759\n",
      "run 1  stream  4\n",
      "total_loss=12.556123\n",
      "run 1  stream  5\n",
      "total_loss=13.274386\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,33.3975780676403] \n",
      "    LOG    range before scaling: [0.0,3.5379861568903475]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5380) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5380) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5380) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5380) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5380) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5380) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=5.709332\n",
      "run 1  stream  1\n",
      "total_loss=9.255465\n",
      "run 1  stream  2\n",
      "total_loss=11.762305\n",
      "run 1  stream  3\n",
      "total_loss=13.124741\n",
      "run 1  stream  4\n",
      "total_loss=13.887143\n",
      "run 1  stream  5\n",
      "total_loss=14.637705\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,33.13585268098736] \n",
      "    LOG    range before scaling: [0.0,3.5303482302268483]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5303) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5303) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5303) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5303) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5303) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5303) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=6.369596\n",
      "run 1  stream  1\n",
      "total_loss=10.867779\n",
      "run 1  stream  2\n",
      "total_loss=13.980455\n",
      "run 1  stream  3\n",
      "total_loss=15.542000\n",
      "run 1  stream  4\n",
      "total_loss=16.371202\n",
      "run 1  stream  5\n",
      "total_loss=17.225426\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,33.280131210784056] \n",
      "    LOG    range before scaling: [0.0,3.534565921341392]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5346) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5346) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5346) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5346) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5346) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5346) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=5.609544\n",
      "run 1  stream  1\n",
      "total_loss=9.418939\n",
      "run 1  stream  2\n",
      "total_loss=12.076977\n",
      "run 1  stream  3\n",
      "total_loss=13.472772\n",
      "run 1  stream  4\n",
      "total_loss=14.229120\n",
      "run 1  stream  5\n",
      "total_loss=15.087184\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,32.11532777519478] \n",
      "    LOG    range before scaling: [0.0,3.4999962499762787]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5000) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5000) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5000) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5000) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5000) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.5000) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=6.821433\n",
      "run 1  stream  1\n",
      "total_loss=11.358891\n",
      "run 1  stream  2\n",
      "total_loss=14.132816\n",
      "run 1  stream  3\n",
      "total_loss=15.642191\n",
      "run 1  stream  4\n",
      "total_loss=16.511948\n",
      "run 1  stream  5\n",
      "total_loss=17.409472\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,31.69168979132725] \n",
      "    LOG    range before scaling: [0.0,3.487120910783685]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4871) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4871) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4871) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4871) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4871) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4871) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=7.999327\n",
      "run 1  stream  1\n",
      "total_loss=13.482128\n",
      "run 1  stream  2\n",
      "total_loss=16.723835\n",
      "run 1  stream  3\n",
      "total_loss=18.403437\n",
      "run 1  stream  4\n",
      "total_loss=19.433901\n",
      "run 1  stream  5\n",
      "total_loss=20.425400\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,30.8512792035636] \n",
      "    LOG    range before scaling: [0.0,3.4610775445888384]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4611) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4611) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4611) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4611) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4611) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4611) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=7.980068\n",
      "run 1  stream  1\n",
      "total_loss=13.530258\n",
      "run 1  stream  2\n",
      "total_loss=16.600834\n",
      "run 1  stream  3\n",
      "total_loss=18.222290\n",
      "run 1  stream  4\n",
      "total_loss=19.167761\n",
      "run 1  stream  5\n",
      "total_loss=20.117311\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,31.324305358761944] \n",
      "    LOG    range before scaling: [0.0,3.475819435138916]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4758) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4758) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4758) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4758) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4758) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.4758) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=6.953572\n",
      "run 1  stream  1\n",
      "total_loss=11.759377\n",
      "run 1  stream  2\n",
      "total_loss=14.617257\n",
      "run 1  stream  3\n",
      "total_loss=16.164173\n",
      "run 1  stream  4\n",
      "total_loss=17.096838\n",
      "run 1  stream  5\n",
      "total_loss=18.026779\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,27.730657520756473] \n",
      "    LOG    range before scaling: [0.0,3.357964758937335]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.3580) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.3580) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.3580) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.3580) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.3580) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.3580) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=9.278730\n",
      "run 1  stream  1\n",
      "total_loss=15.425681\n",
      "run 1  stream  2\n",
      "total_loss=18.812243\n",
      "run 1  stream  3\n",
      "total_loss=20.595428\n",
      "run 1  stream  4\n",
      "total_loss=21.647478\n",
      "run 1  stream  5\n",
      "total_loss=22.720100\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,27.2506662932676] \n",
      "    LOG    range before scaling: [0.0,3.3411170429167067]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.3411) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.3411) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.3411) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.3411) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.3411) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.3411) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=9.569515\n",
      "run 1  stream  1\n",
      "total_loss=15.844178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 1  stream  2\n",
      "total_loss=19.388672\n",
      "run 1  stream  3\n",
      "total_loss=21.259665\n",
      "run 1  stream  4\n",
      "total_loss=22.350668\n",
      "run 1  stream  5\n",
      "total_loss=23.400372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f94f26ca190>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# samples have the same cf (00.00) and irreg (00.00), but different rate parameters\n",
    "testfolder = 'inputs/Pops'\n",
    "import os\n",
    "\n",
    "input_imgs = []\n",
    "loss=[]\n",
    "listdir = os.listdir(testfolder)\n",
    "print(listdir)\n",
    "for filename in listdir:\n",
    "    if 'cf' in filename:\n",
    "        input_imgs.append(testfolder+'/'+filename)\n",
    "    \n",
    "\n",
    "input_imgs.sort()\n",
    "print(\"input_imgs:\",input_imgs)\n",
    "\n",
    "styleloss_mat = np.zeros((21,1))\n",
    "\n",
    "for i in range(len(input_imgs)):\n",
    "    style_img,N_SAMPLES,N_FREQ=prepare_input(input_imgs[0])\n",
    "    input_img,n_SAMPLES,n_FREQ=prepare_input(input_imgs[i]) #create two dummy variables\n",
    "    styleloss_mat[i,0] = run_style_transfer(cnnlist, style_img, input_img)\n",
    "    loss.append(styleloss_mat[i])\n",
    "        \n",
    "plt.figure()\n",
    "plt.imshow(styleloss_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_loss(styleloss_mat,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['popcf02.wav', 'popcf16.wav', 'popb2.wav', 'popb3.wav', 'popcf17.wav', 'popcf15.wav', 'popcf01.wav', 'popb1.wav', 'popb0.wav', 'popc4.wav', 'popcf00.wav', 'popcf14.wav', 'popcf10.wav', 'popcf04.wav', '.DS_Store', 'popc0.wav', 'popb4.wav', 'popc1.wav', 'popcf05.wav', 'popcf11.wav', 'popcf07.wav', 'popcf13.wav', 'popcf03wav.wav', 'popc3.wav', 'popc2.wav', 'popcf12.wav', 'popcf06.wav', 'poprate08.wav', 'poprate09.wav', 'poprate19.wav', 'poprate18.wav', 'poprate15.wav', 'poprate01.wav', 'poprate00.wav', 'poprate14.wav', 'poprate02.wav', 'poprate16.wav', 'poprate17.wav', 'poprate03.wav', 'poprate07.wav', 'poprate13.wav', 'poprate12.wav', 'poprate06.wav', 'poprate10.wav', 'poprate04.wav', 'poprate05.wav', 'poprate11.wav', '.ipynb_checkpoints', 'popcf20.wav', 'popcf08.wav', 'popcf09.wav', 'popcf19.wav', 'popcf18.wav']\n",
      "input_imgs: ['inputs/Pops/poprate00.wav', 'inputs/Pops/poprate01.wav', 'inputs/Pops/poprate02.wav', 'inputs/Pops/poprate03.wav', 'inputs/Pops/poprate04.wav', 'inputs/Pops/poprate05.wav', 'inputs/Pops/poprate06.wav', 'inputs/Pops/poprate07.wav', 'inputs/Pops/poprate08.wav', 'inputs/Pops/poprate09.wav', 'inputs/Pops/poprate10.wav', 'inputs/Pops/poprate11.wav', 'inputs/Pops/poprate12.wav', 'inputs/Pops/poprate13.wav', 'inputs/Pops/poprate14.wav', 'inputs/Pops/poprate15.wav', 'inputs/Pops/poprate16.wav', 'inputs/Pops/poprate17.wav', 'inputs/Pops/poprate18.wav', 'inputs/Pops/poprate19.wav']\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7582) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7582) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7582) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7582) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7582) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input range: tensor(3.7582) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=0.000000\n",
      "run 1  stream  1\n",
      "total_loss=0.000000\n",
      "run 1  stream  2\n",
      "total_loss=0.000000\n",
      "run 1  stream  3\n",
      "total_loss=0.000000\n",
      "run 1  stream  4\n",
      "total_loss=0.000000\n",
      "run 1  stream  5\n",
      "total_loss=0.000000\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,43.474515293930494] \n",
      "    LOG    range before scaling: [0.0,3.794916335098593]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7949) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7949) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7949) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7949) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7949) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7949) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=0.211803\n",
      "run 1  stream  1\n",
      "total_loss=0.389288\n",
      "run 1  stream  2\n",
      "total_loss=0.493742\n",
      "run 1  stream  3\n",
      "total_loss=0.590978\n",
      "run 1  stream  4\n",
      "total_loss=0.622421\n",
      "run 1  stream  5\n",
      "total_loss=0.732613\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,41.04187113500781] \n",
      "    LOG    range before scaling: [0.0,3.7386660534628033]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7387) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7387) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7387) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7387) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7387) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7387) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=0.377785\n",
      "run 1  stream  1\n",
      "total_loss=0.619695\n",
      "run 1  stream  2\n",
      "total_loss=0.819779\n",
      "run 1  stream  3\n",
      "total_loss=0.955692\n",
      "run 1  stream  4\n",
      "total_loss=1.094379\n",
      "run 1  stream  5\n",
      "total_loss=1.327371\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,42.39904048677923] \n",
      "    LOG    range before scaling: [0.0,3.7704373322624036]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7704) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7704) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7704) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7704) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7704) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7704) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=0.791883\n",
      "run 1  stream  1\n",
      "total_loss=1.339267\n",
      "run 1  stream  2\n",
      "total_loss=1.779947\n",
      "run 1  stream  3\n",
      "total_loss=2.121986\n",
      "run 1  stream  4\n",
      "total_loss=2.259759\n",
      "run 1  stream  5\n",
      "total_loss=2.479069\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,44.563522050285094] \n",
      "    LOG    range before scaling: [0.0,3.8191074413479615]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8191) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8191) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8191) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8191) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8191) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8191) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=2.217015\n",
      "run 1  stream  1\n",
      "total_loss=3.069397\n",
      "run 1  stream  2\n",
      "total_loss=3.524916\n",
      "run 1  stream  3\n",
      "total_loss=3.812074\n",
      "run 1  stream  4\n",
      "total_loss=4.013913\n",
      "run 1  stream  5\n",
      "total_loss=4.308272\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,42.42260151183797] \n",
      "    LOG    range before scaling: [0.0,3.770980077713762]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7710) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7710) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7710) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7710) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7710) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7710) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=3.473027\n",
      "run 1  stream  1\n",
      "total_loss=5.529660\n",
      "run 1  stream  2\n",
      "total_loss=7.029041\n",
      "run 1  stream  3\n",
      "total_loss=8.238798\n",
      "run 1  stream  4\n",
      "total_loss=9.062850\n",
      "run 1  stream  5\n",
      "total_loss=9.936972\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,41.57954786596249] \n",
      "    LOG    range before scaling: [0.0,3.7513740409487055]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7514) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7514) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7514) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7514) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7514) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7514) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=5.901302\n",
      "run 1  stream  1\n",
      "total_loss=9.534828\n",
      "run 1  stream  2\n",
      "total_loss=12.213048\n",
      "run 1  stream  3\n",
      "total_loss=14.210906\n",
      "run 1  stream  4\n",
      "total_loss=15.411467\n",
      "run 1  stream  5\n",
      "total_loss=16.681078\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,42.30645686056237] \n",
      "    LOG    range before scaling: [0.0,3.7683017430728634]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7683) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7683) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7683) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7683) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7683) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7683) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=4.242632\n",
      "run 1  stream  1\n",
      "total_loss=7.094559\n",
      "run 1  stream  2\n",
      "total_loss=9.405856\n",
      "run 1  stream  3\n",
      "total_loss=10.868106\n",
      "run 1  stream  4\n",
      "total_loss=12.092662\n",
      "run 1  stream  5\n",
      "total_loss=13.326310\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,45.4666168506122] \n",
      "    LOG    range before scaling: [0.0,3.8387341375762225]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8387) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8387) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8387) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8387) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8387) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8387) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=8.362962\n",
      "run 1  stream  1\n",
      "total_loss=13.779163\n",
      "run 1  stream  2\n",
      "total_loss=17.693291\n",
      "run 1  stream  3\n",
      "total_loss=20.482683\n",
      "run 1  stream  4\n",
      "total_loss=23.220932\n",
      "run 1  stream  5\n",
      "total_loss=25.946791\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,44.495531460688] \n",
      "    LOG    range before scaling: [0.0,3.817614111478812]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8176) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8176) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8176) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8176) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8176) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8176) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=12.655468\n",
      "run 1  stream  1\n",
      "total_loss=21.368273\n",
      "run 1  stream  2\n",
      "total_loss=28.186253\n",
      "run 1  stream  3\n",
      "total_loss=32.988468\n",
      "run 1  stream  4\n",
      "total_loss=37.240906\n",
      "run 1  stream  5\n",
      "total_loss=40.851402\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,41.65011913198725] \n",
      "    LOG    range before scaling: [0.0,3.7530300671808505]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7530) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7530) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7530) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7530) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7530) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7530) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=17.543886\n",
      "run 1  stream  1\n",
      "total_loss=29.792122\n",
      "run 1  stream  2\n",
      "total_loss=39.509468\n",
      "run 1  stream  3\n",
      "total_loss=46.695377\n",
      "run 1  stream  4\n",
      "total_loss=52.762272\n",
      "run 1  stream  5\n",
      "total_loss=58.851303\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,43.05472468996687] \n",
      "    LOG    range before scaling: [0.0,3.785432604064502]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7854) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7854) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7854) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7854) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7854) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7854) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=25.979223\n",
      "run 1  stream  1\n",
      "total_loss=43.623505\n",
      "run 1  stream  2\n",
      "total_loss=58.322231\n",
      "run 1  stream  3\n",
      "total_loss=70.250603\n",
      "run 1  stream  4\n",
      "total_loss=82.426849\n",
      "run 1  stream  5\n",
      "total_loss=93.766159\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,43.46055766825932] \n",
      "    LOG    range before scaling: [0.0,3.7946024516031196]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7946) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7946) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7946) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7946) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7946) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7946) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=33.337414\n",
      "run 1  stream  1\n",
      "total_loss=55.517666\n",
      "run 1  stream  2\n",
      "total_loss=72.787857\n",
      "run 1  stream  3\n",
      "total_loss=86.677231\n",
      "run 1  stream  4\n",
      "total_loss=102.367645\n",
      "run 1  stream  5\n",
      "total_loss=116.009331\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,45.69162942427914] \n",
      "    LOG    range before scaling: [0.0,3.843564907159058]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8436) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8436) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8436) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8436) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8436) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8436) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=40.562962\n",
      "run 1  stream  1\n",
      "total_loss=67.949287\n",
      "run 1  stream  2\n",
      "total_loss=90.031433\n",
      "run 1  stream  3\n",
      "total_loss=109.632828\n",
      "run 1  stream  4\n",
      "total_loss=128.281174\n",
      "run 1  stream  5\n",
      "total_loss=147.144409\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,43.84131780095982] \n",
      "    LOG    range before scaling: [0.0,3.8031299867319652]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8031) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8031) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8031) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8031) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8031) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8031) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=39.795948\n",
      "run 1  stream  1\n",
      "total_loss=66.519356\n",
      "run 1  stream  2\n",
      "total_loss=89.196068\n",
      "run 1  stream  3\n",
      "total_loss=109.470421\n",
      "run 1  stream  4\n",
      "total_loss=129.698120\n",
      "run 1  stream  5\n",
      "total_loss=148.108582\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,42.49621346564377] \n",
      "    LOG    range before scaling: [0.0,3.772673887539039]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7727) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7727) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7727) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7727) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7727) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7727) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=67.112572\n",
      "run 1  stream  1\n",
      "total_loss=110.973457\n",
      "run 1  stream  2\n",
      "total_loss=147.659943\n",
      "run 1  stream  3\n",
      "total_loss=184.290619\n",
      "run 1  stream  4\n",
      "total_loss=220.765259\n",
      "run 1  stream  5\n",
      "total_loss=259.129272\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [2.067866220292068e-08,44.08617177486845] \n",
      "    LOG    range before scaling: [2.0678661989117146e-08,3.808575586962767]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8086) tensor(2.0679e-08)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8086) tensor(2.0679e-08)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8086) tensor(2.0679e-08)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8086) tensor(2.0679e-08)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8086) tensor(2.0679e-08)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8086) tensor(2.0679e-08)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=83.372940\n",
      "run 1  stream  1\n",
      "total_loss=139.976685\n",
      "run 1  stream  2\n",
      "total_loss=185.215668\n",
      "run 1  stream  3\n",
      "total_loss=233.856735\n",
      "run 1  stream  4\n",
      "total_loss=281.417999\n",
      "run 1  stream  5\n",
      "total_loss=329.495819\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [2.507127162373394e-07,44.60316650083154] \n",
      "    LOG    range before scaling: [2.507126848089116e-07,3.819977154917148]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8200) tensor(2.5071e-07)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8200) tensor(2.5071e-07)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8200) tensor(2.5071e-07)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8200) tensor(2.5071e-07)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8200) tensor(2.5071e-07)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.8200) tensor(2.5071e-07)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=109.375458\n",
      "run 1  stream  1\n",
      "total_loss=184.843903\n",
      "run 1  stream  2\n",
      "total_loss=246.856598\n",
      "run 1  stream  3\n",
      "total_loss=322.528748\n",
      "run 1  stream  4\n",
      "total_loss=389.100403\n",
      "run 1  stream  5\n",
      "total_loss=462.272888\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [1.4859129980207335e-07,43.492421323248635] \n",
      "    LOG    range before scaling: [1.4859128876238725e-07,3.795318867325525]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7953) tensor(1.4859e-07)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7953) tensor(1.4859e-07)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7953) tensor(1.4859e-07)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7953) tensor(1.4859e-07)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7953) tensor(1.4859e-07)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7953) tensor(1.4859e-07)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=152.688553\n",
      "run 1  stream  1\n",
      "total_loss=257.675690\n",
      "run 1  stream  2\n",
      "total_loss=344.485168\n",
      "run 1  stream  3\n",
      "total_loss=449.001587\n",
      "run 1  stream  4\n",
      "total_loss=546.568909\n",
      "run 1  stream  5\n",
      "total_loss=638.976257\n",
      "raw spectrogram R range before log and scaling: [0.0,41.871869674554475] \n",
      "    LOG    range before scaling: [0.0,3.758215892314689]\n",
      "shape of a_style is (257, 247)\n",
      "raw spectrogram R range before log and scaling: [0.0,42.55690134249763] \n",
      "    LOG    range before scaling: [0.0,3.774068160146493]\n",
      "shape of a_style is (257, 247)\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7741) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7741) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 4), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7741) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 8), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7741) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 16), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7741) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 64), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Building the style transfer model..\n",
      "Input range: tensor(3.7741) tensor(0.)\n",
      "Sequential(\n",
      "  (conv_1): Conv2d(257, 512, kernel_size=(1, 128), stride=(1, 1), bias=False)\n",
      "  (relu_1): ReLU()\n",
      "  (style_loss_1): StyleLoss(\n",
      "    (gram): GramMatrix()\n",
      "    (criterion): MSELoss()\n",
      "  )\n",
      ")\n",
      "Created 5 MS models\n",
      "Current Run Number =  1\n",
      "numStreams =  6\n",
      "run 1  stream  0\n",
      "total_loss=177.052536\n",
      "run 1  stream  1\n",
      "total_loss=297.634216\n",
      "run 1  stream  2\n",
      "total_loss=402.421387\n",
      "run 1  stream  3\n",
      "total_loss=529.390015\n",
      "run 1  stream  4\n",
      "total_loss=669.617737\n",
      "run 1  stream  5\n",
      "total_loss=785.783630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f94dbc86b90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAD4AAAD4CAYAAAC0cFXtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAALLUlEQVR4nO2dfYxVxRmHnx/sLsuXIBIVEYtaSkSj2BLUmCYqfiAxaj/SQhqLra2t0UQTm9TapDb2H22jNi0G6gdFG6u0fpWkVEXUWlOlIEGEAoLEpisEWqGAIOC6b/+YWTh7OXfv2XPu7j3uzJNs7r0zc+7Mb+fcc+e+73nfkZkRIgMaPYBGEYWHRhQeGk2NHkAaLRpkrQyt2W4/ezloB5Snj1IKb2Uo52hazXbLbGnuPgqd6pKmS9ogaZOk21LqB0la6OuXSRpfpL96klu4pIHA/cDlwCRglqRJFc2uA3aa2WeB+4C78/ZXb4rM+FRgk5ltNrODwBPAVRVtrgIe8c+fBKZJyvWZrDdFhI8F/p143ebLUtuYWTuwCzimQJ91o8jFLW3mKhf+Wdq4htL1wPUArQwpMKxsFJnxNmBc4vWJwJZqbSQ1ASOAHWlvZmYPmNkUM5vSzKACw8pGEeHLgQmSTpbUAswEFlW0WQTM9s+/CrxkJfk5mPtUN7N2STcBzwMDgflmtlbSncAKM1sEPAz8TtIm3EzPrMeg64FKMgFdOEqjLOsCZrftyPUtEexaPQoPjSg8NKLw0IjCQyMKD40oPDSi8NCIwkOjiENhnKSXJa2TtFbSzSltLpC0S9Iq//eTYsOtH0XMy+3ArWa2UtJw4E1JS8zsnxXt/mZmVxTop1fIPeNmttXMVvrne4B1HOlQKC11+Yx7Z+DZwLKU6vMkvSXpL5JO7+Y9rpe0QtKKjzlQj2F1S2E3saRhwFPALWa2u6J6JfAZM/tQ0gzgWWBC2vuY2QPAA+CsrEXHVYuibuJmnOjHzOzpynoz221mH/rni4FmSaOL9FkvilzVhXMYrDOze6u0Ob7TOyppqu/vg7x91pMip/r5wDXA25JW+bLbgZMAzGwezm10g6R24CNgZn9wIb1Gujc02WYOMCdvH71JXLmFRhQeGsEKL+UNfgBkuTmqwBdjsDMehYdGFB4aUXhoROGhUc6Vm0ADB9ZuF1duPaewcEnvSXrbe0pWpNRL0q98XMpqSZ8v2mc9qNepfqGZ/bdK3eU4k/IE4Bxgrn9sKH1xql8FPGqON4CRksb0Qb/dUg/hBrwg6U0fXlFJltiVrp4U+xR4UoDzzWyLpGOBJZLWm9mrifpMcSldPCkDSu5JATCzLf5xO/AMLiwrSZbYlT6nqAtpqHcRI2kocCmwpqLZIuCb/up+LrDLzLYW6bceFD3VjwOe8V6iJuD3ZvacpO/DIW/KYmAGsAnYB3yrYJ91oZBwM9sMnJVSPi/x3IAbe/bOgiwrt/b8QYtx5RYaUXhoROGhEYWHRhQeGuU0NgJ09O4v02BnPAoPjSg8NKLwniJpYiLWZJWk3ZJuqWjT/2JSzGwDMBkOZfp6H2dlraR/xaRUMA1418z+Vaf363XqtXKbCTxepe48SW/hbOk/MLO1aY26JLnSUNTSXLNTNdLY6PM8XQn8MaW6MyblLODXuJiUVJJJrlrUWnRYNanHqX45sNLMtlVW9MuYlASzqHKa99eYFCQNAS4BvpcoS3pR+l9MCoCZ7aMiFWGFFyXGpJSNKDw0ghVeTpvbAKHWDJk690c3cY+JwkMjCg+NKDw0ovDQCFZ4OZesGoBaM9jdlH/egp3xTMIlzZe0XdKaRNkoSUskbfSPR1c5drZvs1HS7LQ2jSDrjC8ApleU3QYsNbMJwFL/uguSRgF34GJQpgJ3VPsH9TWZhPuIg8pNH5J7oDwCXJ1y6GXAEjPbYWY7gSUc+Q9sCEU+48d13nDvH49NaZMpHgW6xqQc7PiowLCy0dsXt8z7pHTxpAwY3MvDKiZ8W2cYlX/cntKmlPEoUEx4cg+U2cCfUto8D1wq6Wh/UbvUlzWcrF9njwOvAxMltUm6DrgLuETSRpw35S7fdoqkhwDMbAfwM9xmMsuBO31ZwynlPikjBo+x806pHbPz+ubfsuujrXGflJ4QhYdGFB4aUXhoROGhEazwUhobbYDoaG3J1C4vwc54FB4aUXhoROHVqOJF+YWk9T5b1zOSRlY5ttvMX40ky4wv4EgnwBLgDDM7E3gH+FE3x19oZpPNbEq+IfYONYWneVHM7AW/kTrAGziz8aeKeqzcvg0srFLXmfnLgN/4RFapJGNSBg0awSfDaq/cKLByK3qj/o9xWwM9VqVJrcxfh+iS3Wv42PJm9/Iu3yuAb1SLOsiQ+ath5BIuaTrwQ+BKH6WQ1iZL5q+GkeXrLM2LMgcYjjt9V0ma59ueIGmxP/Q44DUfc/YP4M9m9lyvqMhBzc+4mc1KKX64StstuBRmVTN/lYW4cguNKDw0SmlzA3p9SoKd8Sg8NKLw0IjCQyMKD41ghZdyydreOoCdn6sdk9K+Jsak9Ji8npSfSno/kbVrRpVjp0va4PdIOSJ0o5Hk9aQA3Oc9JJN9Vp8u+Ixf9+MyA00CZkmaVGSw9SSXJyUjU4FNZrbZzA4CT+DiWEpBkc/4Td5pOL9KZFHmeBToGpPSvn9vgWFlI6/wucCpuER2W4F7UtpkjkeBrjEpTa1Dcw4rO7mEm9k2M/vEzDqAB0n3kJQ2HgXye1KSexl9iXQPyXJggqSTfc63mbg4llJQcwHjPSkXAKMlteEiBy+QNBl36r6Hz+4l6QTgITObYWbtkm7CBd8MBOZXy97XCHrNk+JfL8ZtENMjOpph37G1XcAdtbMbViWu3EIjCg+NKDw0ovDQCFZ4KW1u1trBgdNqp0uw1o7cfQQ741F4aEThoRGFh0YW09N83O3Z283sDF+2EJjom4wE/mdmk1OOfQ/YA3wCtJcpPCPLAmYB7m7lRzsLzOzrnc8l3QPs6ub47najbxhZbG6vShqfVuf3R/gacFF9h9X7FF2yfhHYZmYbq9TnikkZMWYw153595qdzx2c3+NSVHjVHTM8uWJSxp4+stQxKU3Al6kegdT/YlI8FwPrzawtrbK/xqRAyv5HIcSkYGbXppTFmJSyE4WHRrDCS2lsHNO0n9tHb6jZ7tmm/bn7CHbGo/DQiMJDIwoPjSg8NIIVXsol6zurh3DZCUeY6Y9sZ/k3vc1igRkn6WVJ6yStlXSzL+/324W0A7ea2WnAucCNPsSif28XYmZbzWylf74HWIeLNAhnuxDvUTkbWEadtwvpazILlzQMeAq4xcx2Zz0spSzVWZCMSfmYA1mHlZusm0c040Q/ZmZP++K6bheSjElpJsNmrQXJclUX7sb8dWZ2b6Kq328Xcj5wDXBRRWRh3C6k3hylUXaOptVst8yWstt25ErjV0rhkvYAldbG0UDlDQYTzWx4nj5KuWQFNlTeNiJpRVpZ3g6C/ZEShZeMtFtGspZlopQXt76grDPe60ThjaTCqLHSPx7KK5HINWGStkraKWmvpGWSxku6VtJ/EivL79Ts1Mwa/gf8HGfIGAh8AMwDWoC3gDOAd4FTgA9xP3wW+uNm4u66uhaY05M+SzHjHDZqTAVW424D7cwrcSM+14Rvu4/DOSueBGqvbVMoi/BOo8ZY3Ox2GjXagPEcNma0+tdfkXS1T3W+Cxjmy1ZLelJS8qdwKn0mXNKLktak/CUzhdT6wXES7nf/X4FfSjrVlz8HjPeJ7l/ksEmsKn22Vjezi6vVSeo0arThkm50GjVOxGUkOMW/xxZJ+3A/Vl4BvgCMAN5NZPx+ELi71njKcqp3GjWWA2cCryTySszF5Zo4y5u/hgBjcHaCccBLwPGJ97oSZxDtllKs3CQdA/wBdyrvxX1mhbPWtODEXgIcBewEBuO+AbbgrLbfxQlux134bjCz9d32WQbhjaAsp3qfE4WHRhQeGlF4aPwfqPm+UA0o1GUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# samples have the same cf (00.00) and irreg (00.00), but different rate parameters\n",
    "testfolder = 'inputs/Pops'\n",
    "import os\n",
    "\n",
    "input_imgs = []\n",
    "loss=[]\n",
    "listdir = os.listdir(testfolder)\n",
    "print(listdir)\n",
    "for filename in listdir:\n",
    "    if 'rate' in filename:\n",
    "        input_imgs.append(testfolder+'/'+filename)\n",
    "    \n",
    "\n",
    "input_imgs.sort()\n",
    "print(\"input_imgs:\",input_imgs)\n",
    "\n",
    "styleloss_mat = np.zeros((21,1))\n",
    "\n",
    "for i in range(len(input_imgs)):\n",
    "    style_img,N_SAMPLES,N_FREQ=prepare_input(input_imgs[0])\n",
    "    input_img,n_SAMPLES,n_FREQ=prepare_input(input_imgs[i]) #create two dummy variables\n",
    "    styleloss_mat[i,0] = run_style_transfer(cnnlist, style_img, input_img)\n",
    "    loss.append(styleloss_mat[i])\n",
    "        \n",
    "plt.figure()\n",
    "plt.imshow(styleloss_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_loss(styleloss_mat,loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
